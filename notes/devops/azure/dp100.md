# Designing and implementing a data science solution on Azure

## Explore and configure the Azure Machine Learning workspace 

### 1. Azure Machine Learning Platform

- **Azure Machine Learning (AML)**  
  A cloud-based platform that provides tools and services for data scientists to train, deploy, and manage machine learning models on Microsoft Azure.

- **Azure Machine Learning Workspace**  
  The central top-level resource in AML where all resources and assets (datasets, models, compute, etc.) are organized and managed for machine learning projects.

- **Azure Machine Learning Studio**  
  A web portal interface launched from the Azure portal to create, manage, and use Azure Machine Learning resources and assets.

- **Azure Machine Learning Service**  
  The service in an Azure subscription that allows creation and management of machine learning workspaces and resources.

- **Azure Machine Learning Studio**
  The Azure Machine Learning studio is a web portal, which provides an overview of all resources and assets available in the workspace. The menu shows what you can do in the studio (Author, Assets, Manage).

- **Python Software Development Kit (SDK)**
  Azure Machine Learning offers a software development kit (SDK) so that you can interact with the workspace using Python (Jupyter notebooks, VS Code...). To efficiently work with the Python SDK, you'll need to use the reference documentation.

- **Azure Command-Line Interface (CLI)**
  Another code-based approach to interact with the Azure Machine Learning workspace is the command-line interface (CLI). To use the Azure CLI to interact with the Azure Machine Learning workspace, you'll use commands. You can find the list of commands in the reference documentation of the CLI.

### 2. Compute Services

- **Compute**  
  The cloud infrastructure that provides the computational power needed to run machine learning workflows (training, inference, etc.).

- **Compute Instance**  
   Avirtual machine (VM) in Azure specifically provisioned for individual development and experimentation (on subset of training data). As a data scientist, you can attach a compute instance to your notebooks to run code interactively. Compute instances are dedicated to a single user, so to avoid unnecessary costs, it’s best to stop them when not in use.

- **Compute Clusters**  
  Scalable, on-demand clusters of CPU or GPU nodes used to distribute and accelerate training jobs in the cloud. Clusters allow you to use parallel processing to distribute the workload and reduce the time it takes to run a script. When you create a compute cluster, there are three main parameters you need to consider: size, max_instances and tier. If you want an alternative that you don't have to create and manage, you can use Azure Machine Learning's serverless compute.

- **Kubernetes Clusters**  
  Azure Kubernetes Service (AKS) clusters used to orchestrate containers for scalable model deployment or batch inferencing.

- **Attached Computes**  
  External compute resources, like an existing AKS cluster, connected to the Azure Machine Learning workspace to run jobs.

- **Serverless Computes**  
  Compute resources that run workloads without the user managing the underlying infrastructure, typically for lightweight or event-driven ML tasks.

![image](https://github.com/user-attachments/assets/bc83f3ca-e11e-48f2-a578-8f1e3fe8e6eb)


### 3. Storage and Data Management

- **Datastore**  
  References or connections to Azure data storage services (e.g., Blob Storage, Data Lake) where data is stored outside the workspace. They encapsulate the information needed to connect to data sources, and securely store this connection information so that you don’t have to code it in your scripts. Authentication methods depend on storage service (e.g., Credential-based, Identity-based).

- **Azure Storage Account**  
  An Azure resource created alongside the workspace, providing persistent storage (blobs, files) used by the workspace for datasets, outputs, and other artifacts.

- **Data Asset**  
  A specific file or folder registered in the workspace that points to actual data within datastores or storage services. Data assets are references to where the data is stored, how to get access, and any other relevant metadata. There are three main types of data assets you can use: URI file, URI folder and MLTable (Points to a folder or file, and includes a schema to read as tabular data. Use when the schema of your data is complex or changes frequently).

- **Azure Key Vault**  
  A secure service where sensitive information like connection strings, secrets, and credentials are stored. Used to protect datastore connection information.

- **Uniform Resource Identifiers (URIs)**
  A URI references the location of your data. For Azure Machine Learning to connect to your data, you need to prefix the URI with the appropriate protocol: http(s) (Azure Blob Storage or publicly available data), abfs(s) (Azure Data Lake Storage Gen 2), azureml (datastore).

### 4. Identity and Access Management

- **Azure Portal**  
  The primary web interface for managing all Azure services, including the Azure Machine Learning workspace and resources.

- **Access Control**  
  Mechanism to assign and manage permissions for users or teams to access Azure resources.

- **Role-Based Access Control (RBAC)**  
  Azure’s authorization system to manage access by assigning roles to users/groups, controlling who can view or manage specific resources such as the AML workspace or compute.

### 5. Development and Deployment Assets

- **Models**  
  The trained machine learning artifacts created with frameworks like Scikit-learn or PyTorch, registered in the workspace with name and version for deployment and tracking.

- **Azure Machine Learning Environments**  
  Define the software dependencies, environment variables, and configurations required to run machine learning scripts or code on compute resources (docker images + conda environments). Stored as container images in Azure Container Registry. Curated environments are prebuilt environments for the most common machine learning workloads, available in your workspace by default. When you need to create your own environment in Azure Machine Learning to list all necessary packages, libraries, and dependencies to run your scripts, you can create custom environments. You can define an environment from a Docker image, a Docker build context, and a conda specification with Docker image.

- **Components**  
  Reusable code units or steps (e.g., data preprocessing, model training) that can be assembled into pipelines within the workspace. Components specify code, environment, and version.

- **Pipelines**  
  Orchestrated workflows built by chaining components to automate complex machine learning tasks.

### 6. Machine Learning Workflow Features

- **Automated Machine Learning (AutoML)**  
  A feature that automates the process of algorithm selection and hyperparameter tuning to find the best performing model for a dataset.

- **Notebooks**  
  Interactive code environments hosted in the workspace, typically run on compute instances, used for exploration and experimentation.

- **Jobs**  
  Executions of scripts or training runs submitted to the Azure Machine Learning workspace. Jobs store all inputs, outputs, and logs for reproducibility.

### 7. Supporting Azure Services

- **Azure Container Registry (ACR)**  
  A private registry service in Azure to store and manage container images, such as those created for environments in AML.

---

### Relationships and Summary

- The **Azure Machine Learning workspace** ties everything together: it contains compute resources (compute instances, clusters, Kubernetes, serverless), data storage (datastores, data assets), and assets (models, environments, components). The easiest and most intuitive way to interact with the Azure Machine Learning workspace, is by using the studio. Though you can use each tool at any time, the studio is ideal for quick experimentation or when you want to explore your past jobs. For more repetitive work (data scientists), or tasks that you'd like to automate (administrators, automation engineers), the Python SDK or Azure CLI are better suited as these tools allow you to define your work in code.

- **Data Management** It's considered a best practice to avoid any sensitive data in your code, like authentication information. Therefore, whenever possible, you should work with datastores and data assets in Azure Machine Learning.

- **Access control (RBAC)** ensures proper permissions for users to interact with these resources safely.

- **Compute** is essential for running experiments, either interactively via notebooks on compute instances or through jobs running scripts on clusters or serverless compute. During experimentation and development, you prefer working in a Jupyter notebook. A notebook experience benefits most from a compute that is continuously running. After experimentation, you can train your models by running Python scripts to prepare for production. You also want the compute target to be ready to handle large volumes of data. When training models with scripts, you want an on-demand compute target. The type of compute you need when using your model to generate predictions depends on whether you want batch or real-time predictions. For batch predictions, you can run a pipeline job in Azure Machine Learning. When you want real-time predictions, you need a type of compute that is running continuously.

- **Environments** package dependencies and configurations needed for consistent execution across compute. As curated environments allow for faster deployment time, it's a best practice to first explore whether one of the pre-created curated environments can be used to run your code. When you need to specify other necessary packages, you can use a curated environment as reference for your own custom environments by modifying the Dockerfiles that back these curated environments.

- **Automated ML** is a high-level feature to speed up model training without manually testing algorithms or hyperparameters.

---

### Details Left Behind or Summarized

#### 1. Specific Mention of "Overview Page"

- The **Overview page** of the workspace in the Azure portal, where you launch Azure Machine Learning Studio, was not explicitly called out.
- This is more of a **UI navigation detail** rather than a core Azure service or concept.

#### 2. Grouping Workspaces by Organizing Principles

- The idea that workspaces can be grouped by:
  - Projects
  - Deployment environments (e.g., test/production)
  - Teams
- This was mentioned but **not listed as a separate concept** because it’s more of a best practice or organizational strategy than a service or feature.

#### 3. Versioning Specifics on Assets and Models

- Versioning for models, environments, components, and data assets was included generally.
- The detailed explanation of **why** versioning is important (e.g., to track specific deployed models or data files) was condensed.

#### 4. Compute Cost Concerns and Role Separation

- The point that compute is the most **cost-intensive resource** was summarized.
- Best practice that **only administrators create/manage compute resources**, while data scientists just use them, was mentioned under governance/access control but not emphasized as a separate best practice.

#### 5. Azure Storage Account Auto-Creation with Workspace

- It was noted that the workspace is linked to an Azure Storage account.
- The detail that **four datastores come pre-added automatically** with the workspace was simplified to just "datastores exist and you can add more."

#### 6. Details on Code Reuse and Components

- Creating components to enable **code reuse across projects** was mentioned.
- Example use cases such as:
  - Normalize data
  - Train regression model
  - Validate model
- These were **not individually called out**.

#### 7. Notebooks Stored in File Share of Azure Storage

- Files created or cloned in notebooks are stored in the **file share of the Azure Storage account** associated with the workspace.
- This storage detail was mentioned briefly but not as a standalone point.

#### 8. Environment Image Storage in Azure Container Registry (ACR)

- The nuance that the environment image is stored in ACR **only when used for the first time** was mentioned.
- This detail was summarized as simply "environments stored in ACR" to keep things less complex.




